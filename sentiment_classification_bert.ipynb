{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Indonesian SMS Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:15:45.375205Z",
     "iopub.status.busy": "2025-06-07T08:15:45.375205Z",
     "iopub.status.idle": "2025-06-07T08:16:03.526921Z",
     "shell.execute_reply": "2025-06-07T08:16:03.526921Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:16:03.549430Z",
     "iopub.status.busy": "2025-06-07T08:16:03.548430Z",
     "iopub.status.idle": "2025-06-07T08:16:03.589691Z",
     "shell.execute_reply": "2025-06-07T08:16:03.588696Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('IDSMSA.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:16:03.591691Z",
     "iopub.status.busy": "2025-06-07T08:16:03.591691Z",
     "iopub.status.idle": "2025-06-07T08:16:03.994506Z",
     "shell.execute_reply": "2025-06-07T08:16:03.993513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check class distribution\n",
    "print(df['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:16:03.996506Z",
     "iopub.status.busy": "2025-06-07T08:16:03.995506Z",
     "iopub.status.idle": "2025-06-07T08:16:04.041048Z",
     "shell.execute_reply": "2025-06-07T08:16:04.041048Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean and map sentiment labels\n",
    "df.dropna(subset=['Sentiment'], inplace=True)\n",
    "df['Sentiment'] = df['Sentiment'].str.lower().str.strip()\n",
    "label_map = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "df['label'] = df['Sentiment'].map(label_map)\n",
    "df.dropna(subset=['label'], inplace=True)\n",
    "df['label'] = df['label'].astype(int)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['Sentence'], \n",
    "    df['label'], \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label']\n",
    ")\n",
    "\n",
    "print(f'Train set size: {len(X_train)}')\n",
    "print(f'Test set size: {len(X_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Direct Evaluation (Without Fine-Tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. IndoBERT (indonesia-bert-sentiment-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:16:04.044048Z",
     "iopub.status.busy": "2025-06-07T08:16:04.043048Z",
     "iopub.status.idle": "2025-06-07T08:16:04.057076Z",
     "shell.execute_reply": "2025-06-07T08:16:04.057076Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_model_name = 'indonesia-bert-sentiment-classification'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)\n",
    "\n",
    "bert_pipeline = pipeline('sentiment-analysis', model=bert_model, tokenizer=bert_tokenizer)\n",
    "\n",
    "# The model labels are different, we need to map them\n",
    "bert_label_map = {'LABEL_0': 'positive', 'LABEL_1': 'neutral', 'LABEL_2': 'negative'}\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "def predict_bert(text):\n",
    "    result = bert_pipeline(text)[0]\n",
    "    return label_map[bert_label_map[result['label']]]\n",
    "\n",
    "bert_preds = [predict_bert(text) for text in tqdm(X_test)]\n",
    "\n",
    "print(\"\\nIndoBERT Classification Report:\")\n",
    "print(classification_report(y_test, bert_preds, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. IndoRoBERTa (indonesian-roberta-base-sentiment-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:16:04.058816Z",
     "iopub.status.busy": "2025-06-07T08:16:04.058816Z",
     "iopub.status.idle": "2025-06-07T08:16:04.072824Z",
     "shell.execute_reply": "2025-06-07T08:16:04.072824Z"
    }
   },
   "outputs": [],
   "source": [
    "roberta_model_name = 'indonesian-roberta-base-sentiment-classifier'\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_model_name)\n",
    "\n",
    "roberta_pipeline = pipeline('sentiment-analysis', model=roberta_model, tokenizer=roberta_tokenizer)\n",
    "\n",
    "def predict_roberta(text):\n",
    "    result = roberta_pipeline(text)[0]\n",
    "    # The labels from this model are already 'positive', 'neutral', 'negative'\n",
    "    return label_map[result['label']]\n",
    "\n",
    "roberta_preds = [predict_roberta(text) for text in tqdm(X_test)]\n",
    "\n",
    "print(\"\\nIndoRoBERTa Classification Report:\")\n",
    "print(classification_report(y_test, roberta_preds, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine-Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:16:04.074822Z",
     "iopub.status.busy": "2025-06-07T08:16:04.074822Z",
     "iopub.status.idle": "2025-06-07T08:16:04.088822Z",
     "shell.execute_reply": "2025-06-07T08:16:04.088822Z"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts.iloc[item])\n",
    "        label = self.labels.iloc[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Fine-Tuning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:16:04.090823Z",
     "iopub.status.busy": "2025-06-07T08:16:04.090823Z",
     "iopub.status.idle": "2025-06-07T08:16:04.103829Z",
     "shell.execute_reply": "2025-06-07T08:16:04.103829Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"labels\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n",
    "\n",
    "def get_predictions(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for d in tqdm(data_loader, desc=\"Predicting\"):\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"labels\"].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            predictions.extend(preds)\n",
    "            real_values.extend(labels)\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return predictions, real_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Fine-Tune IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:16:04.105823Z",
     "iopub.status.busy": "2025-06-07T08:16:04.105823Z",
     "iopub.status.idle": "2025-06-07T08:16:04.197952Z",
     "shell.execute_reply": "2025-06-07T08:16:04.196943Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "bert_ft_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name)\n",
    "bert_ft_model = bert_ft_model.to(device)\n",
    "\n",
    "train_dataset = SentimentDataset(X_train, y_train, bert_tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = SentimentDataset(X_test, y_test, bert_tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(bert_ft_model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        bert_ft_model, train_loader, optimizer, device, scheduler, len(X_train)\n",
    "    )\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "y_pred, y_true = get_predictions(bert_ft_model, test_loader, device)\n",
    "print(\"\\nFine-Tuned IndoBERT Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Fine-Tune IndoRoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:16:04.200953Z",
     "iopub.status.busy": "2025-06-07T08:16:04.200953Z",
     "iopub.status.idle": "2025-06-07T08:16:04.228963Z",
     "shell.execute_reply": "2025-06-07T08:16:04.228963Z"
    }
   },
   "outputs": [],
   "source": [
    "roberta_ft_model = AutoModelForSequenceClassification.from_pretrained(roberta_model_name)\n",
    "roberta_ft_model = roberta_ft_model.to(device)\n",
    "\n",
    "train_dataset_roberta = SentimentDataset(X_train, y_train, roberta_tokenizer)\n",
    "train_loader_roberta = DataLoader(train_dataset_roberta, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset_roberta = SentimentDataset(X_test, y_test, roberta_tokenizer)\n",
    "test_loader_roberta = DataLoader(test_dataset_roberta, batch_size=BATCH_SIZE)\n",
    "\n",
    "optimizer_roberta = torch.optim.AdamW(roberta_ft_model.parameters(), lr=2e-5)\n",
    "total_steps_roberta = len(train_loader_roberta) * EPOCHS\n",
    "scheduler_roberta = get_linear_schedule_with_warmup(\n",
    "    optimizer_roberta,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps_roberta\n",
    ")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        roberta_ft_model, train_loader_roberta, optimizer_roberta, device, scheduler_roberta, len(X_train)\n",
    "    )\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "y_pred_roberta, y_true_roberta = get_predictions(roberta_ft_model, test_loader_roberta, device)\n",
    "print(\"\\nFine-Tuned IndoRoBERTa Classification Report:\")\n",
    "print(classification_report(y_true_roberta, y_pred_roberta, target_names=label_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T08:16:04.232680Z",
     "iopub.status.busy": "2025-06-07T08:16:04.232680Z",
     "iopub.status.idle": "2025-06-07T08:16:04.247323Z",
     "shell.execute_reply": "2025-06-07T08:16:04.244877Z"
    }
   },
   "outputs": [],
    "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get accuracy scores\n",
    "bert_accuracy = accuracy_score(y_test, bert_preds)\n",
    "roberta_accuracy = accuracy_score(y_test, roberta_preds)\n",
    "bert_ft_accuracy = accuracy_score(y_true, y_pred)\n",
    "roberta_ft_accuracy = accuracy_score(y_true_roberta, y_pred_roberta)\n",
    "\n",
    "conclusion_data = {\n",
    "    'Model': ['IndoBERT', 'IndoRoBERTa', 'IndoBERT (Fine-Tuned)', 'IndoRoBERTa (Fine-Tuned)'],\n",
    "    'Accuracy': [bert_accuracy, roberta_accuracy, bert_ft_accuracy, roberta_ft_accuracy]\n",
    "}\n",
    "\n",
    "conclusion_df = pd.DataFrame(conclusion_data)\n",
    "print(conclusion_df)\n",
    "\n",
    "print(\"\\n## Conclusion\\n\")\n",
    "print(\"The table above summarizes the accuracy of the four different approaches. Based on the results, we can determine which model and approach works best for this dataset.\")\n",
    "print(\"\\nAs a reminder, the classification reports are generated using the **test data**. This provides an unbiased evaluation of how the models perform on new, unseen data, which is the standard practice for assessing a model's true performance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
